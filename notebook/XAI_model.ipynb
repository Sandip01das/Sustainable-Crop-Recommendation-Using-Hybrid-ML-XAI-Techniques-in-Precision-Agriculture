{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "data = pd.read_csv('Crop_recommendationV2.csv')\n",
        "\n",
        "# Map Crop Labels to Numbers\n",
        "crop_dict = {\n",
        "    'rice': 1, 'maize': 2, 'jute': 3, 'cotton': 4, 'coconut': 5, 'papaya': 6,\n",
        "    'orange': 7, 'apple': 8, 'muskmelon': 9, 'watermelon': 10, 'grapes': 11,\n",
        "    'mango': 12, 'banana': 13, 'pomegranate': 14, 'lentil': 15, 'blackgram': 16,\n",
        "    'mungbean': 17, 'mothbeans': 18, 'pigeonpeas': 19, 'kidneybeans': 20,\n",
        "    'chickpea': 21, 'coffee': 22\n",
        "}\n",
        "data['label'] = data['label'].map(crop_dict)\n",
        "\n",
        "#Features suggested by Permutation Feature Importance and dropping others.\n",
        "columns_to_drop = [\n",
        "    'soil_moisture', 'soil_type', 'sunlight_exposure', 'wind_speed', 'co2_concentration',\n",
        "    'organic_matter', 'irrigation_frequency', 'crop_density', 'pest_pressure',\n",
        "    'fertilizer_usage', 'growth_stage', 'urban_area_proximity', 'water_source_type',\n",
        "    'frost_risk', 'water_usage_efficiency'\n",
        "]\n",
        "data_crop = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Save and verify new dataset\n",
        "data_crop.to_csv('data_crop.csv', index=False)\n",
        "print(data_crop)\n",
        "\n",
        "# Correlation Heatmap (Excluding Class Label)\n",
        "sns.heatmap(data_crop.drop(columns=['label']).corr(), annot=True, cbar=True)\n",
        "plt.show()\n",
        "\n",
        "# Prepare Data for Training\n",
        "X = data_crop.drop('label', axis=1)\n",
        "y = data_crop['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Normalize Features\n",
        "mx = MinMaxScaler()\n",
        "X_train = mx.fit_transform(X_train)\n",
        "X_test = mx.transform(X_test)\n",
        "\n",
        "# Define Models\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(),\n",
        "    'GaussianNB': GaussianNB(),\n",
        "    'SVC': SVC(),\n",
        "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
        "    'RandomForestClassifier': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "# Evaluate Models\n",
        "metrics = {\n",
        "    \"accuracy\": {},\n",
        "    \"precision\": {},\n",
        "    \"recall\": {},\n",
        "    \"f1_score\": {}\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics[\"accuracy\"][name] = accuracy_score(y_test, y_pred)\n",
        "    metrics[\"precision\"][name] = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    metrics[\"recall\"][name] = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    metrics[\"f1_score\"][name] = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"{name} Model Performance:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy'][name]:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision'][name]:.4f}\")\n",
        "    print(f\"Recall: {metrics['recall'][name]:.4f}\")\n",
        "    print(f\"F1-Score: {metrics['f1_score'][name]:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Detailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=crop_dict.keys(), yticklabels=crop_dict.keys())\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix for {name}')\n",
        "    plt.show()\n",
        "\n",
        "# Plot Accuracy Comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(metrics[\"accuracy\"].keys()), y=list(metrics[\"accuracy\"].values()), palette=\"husl\")\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Accuracy Comparison of Different Models', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision Comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(metrics[\"precision\"].keys()), y=list(metrics[\"precision\"].values()), palette=\"coolwarm\")\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Precision Score', fontsize=12)\n",
        "plt.title('Precision Comparison Across Models', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Plot Recall Comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(metrics[\"recall\"].keys()), y=list(metrics[\"recall\"].values()), palette=\"crest\")\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Recall Score', fontsize=12)\n",
        "plt.title('Recall Comparison Across Models', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Plot F1-Score Comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(metrics[\"f1_score\"].keys()), y=list(metrics[\"f1_score\"].values()), palette=\"magma\")\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('F1-Score', fontsize=12)\n",
        "plt.title('F1-Score Comparison Across Models', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "235zYGUhYHfM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}